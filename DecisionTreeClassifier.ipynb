{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yUOzXh0KEYf"
      },
      "source": [
        "# Data-Mining Course (EECS 6412)\n",
        "# Assignment (II): Decision Tree Classifier Implementation in Python\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065A5OW9KOFA"
      },
      "source": [
        "\n",
        "## Objective: Implement a Decision Tree classifier in Python to gain a deeper understanding of its working principles.\n",
        "\n",
        "**Overal Instructions:**\n",
        "\n",
        "\n",
        "*   Your task is to implement a Decision Tree classifier in Python.\n",
        "*   The implementation has been broken down into multiple subfunctions, each with accompanying hints. Your goal is to complete the code for each function.\n",
        "* You are only allowed to use the **pandas** and **numpy** libraries for this assignment. Some functions from Pandas have been provided for your convenience in the initial section, and you may use them if you feel they are necessary.\n",
        "* Each part of your solution will be graded separately. However the sections are interrelated. It is crucial that your code is well-documented with comments explaining each part of your implementation.\n",
        "* Please be aware that your responses will be thoroughly reviewed to ensure originality. Plagiarized or copied work will result in penalties.\n",
        "\n",
        "\n",
        "**- Please skip the following descriptions and move directly to the Questions section if you are familiar with reading CSV files with Pandas library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBJOIRZSOiDd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Dataset Description for Car Acceptability Classification:\n",
        " Your codes must be general and should work on each tabular datasets with  categorical data types. For this example, have been provided with two datasets for training and testing- a training dataset (1400 samples) and a test dataset (327 samples). Pleased download datasets from [here](https://drive.google.com/drive/folders/1aka1ySucu1e3PqytQnVdEf63v9LT0E5z?usp=sharing). These samples represent the decisions of car experts regarding the acceptability of cars. The experts have categorized the cars into one of four classes: \"acceptable,\" \"unacceptable,\" \"good,\" or \"very good\" based on six categorical features.\n",
        "\n",
        "# Features:\n",
        "\n",
        "* **'BUYING':** This feature determines the purchase price of the car and is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n",
        "\n",
        "* **'MAINTENANCE':** This feature indicates how high the car's maintenance cost is, and it is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n",
        "\n",
        "* **'DOORS':** This featurte indicates number of the doors each car has: '2', '3', '4', '5more'(5 or more than 5 doors).\n",
        "\n",
        "* **'PERSONS':** This feature determines the car's capacity in terms of the number of persons it can accommodate and is categorized as '2', '4', or 'more'.\n",
        "\n",
        "* **'LUG_BOOT':** This feature represents the size of the car's luggage boot (trunk) and is categorized as 'small', 'med' (medium), or 'big'.\n",
        "\n",
        "* **'SAFETY':** This feature provides an estimate of the car's safety level and is categorized as 'low', 'med' (medium), or 'high'.\n",
        "\n",
        "* **'CLASS':** This is the target variable. It indicates the acceptance level of the car and is categorized as 'unacc' (unacceptable), 'acc' (acceptable), 'good', or 'vgood' (very good).\n",
        "\n",
        "**Please note that in this example the \"CLASS\" attribute is located at the last column of the tabular datasets**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8XuTyhMnwP5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Accessing the Datasets:\n",
        "To access and read datasets from Google Drive in Google Colab using the Pandas library, you can follow these steps:\n",
        "\n",
        "1.   Upload CSV Files to Google Drive: First, ensure that you've uploaded the CSV files (train dataset and test dataset) to your Google Drive. You can create a folder for your project and upload the files there.\n",
        "\n",
        "\n",
        "2.   Mount Google Drive in Google Colab:mount your Google Drive using the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AteP7IIqn4XI",
        "outputId": "db77f359-fa10-4e31-c280-25d395d995b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7tuWcl8zVkM"
      },
      "source": [
        "\n",
        "\n",
        "3.   Access and Read Data using Pandas: You can access your CSV files in the mounted Google Drive directory. For example, if your CSV files are located in a folder named \"data-mining/assignment2/UG/\" in your Google Drive, you can read them as follows:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6jZmiO5zwrJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths for your CSV files\n",
        "train_csv_path = '/content/drive/MyDrive/datamining/data_train_c.csv'\n",
        "test_csv_path = '/content/drive/MyDrive/datamining/data_test_c.csv'\n",
        "\n",
        "# Read the data into Pandas DataFrames\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m34_HjXP2PZB"
      },
      "source": [
        "\n",
        "\n",
        "4.   See Some Samples with head() Function:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abW4fCsV2U6O",
        "outputId": "39088ec5-5f5f-463f-caed-ae3a5913530d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples in the Training Dataset:\n",
            "  BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n",
            "0  vhigh         low      2    more      med   high    acc\n",
            "1  vhigh         low  5more       2      med   high  unacc\n",
            "2    med       vhigh      2       2      med    low  unacc\n",
            "3    med         low      2       2      med    low  unacc\n",
            "4    low         med      2       2      big    low  unacc\n",
            "\n",
            "Samples in the Test Dataset:\n",
            "  BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n",
            "0    med       vhigh      2    more    small   high  unacc\n",
            "1    low         low      4       4    small    med    acc\n",
            "2    low         low      4    more    small    low  unacc\n",
            "3  vhigh       vhigh      4       2      big    med  unacc\n",
            "4  vhigh         med  5more       4    small   high    acc\n"
          ]
        }
      ],
      "source": [
        "# See the first 5 samples in the training dataset\n",
        "print(\"Samples in the Training Dataset:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# See the first 5 samples in the test dataset\n",
        "print(\"\\nSamples in the Test Dataset:\")\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xpw1xCD2jM9"
      },
      "source": [
        "\n",
        "\n",
        "5.   Access Feature Names using columns Attribute:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT9x4hgR2sRD",
        "outputId": "45b39250-7cbc-4b13-a718-82704c6bcb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names:\n",
            "Index(['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY',\n",
            "       'CLASS'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Get the feature names (column names) of the training dataset\n",
        "feature_names = train_df.columns\n",
        "print(\"Feature Names:\")\n",
        "print(feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfjUc4iy2wby"
      },
      "source": [
        "6. Access Each Column as a Series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGWxsVWr248i",
        "outputId": "5eab02c7-2e65-4e9f-ce75-22e06a164484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    vhigh\n",
            "1    vhigh\n",
            "2      med\n",
            "3      med\n",
            "4      low\n",
            "Name: BUYING, dtype: object\n",
            "0      low\n",
            "1      low\n",
            "2    vhigh\n",
            "3      low\n",
            "4      med\n",
            "Name: MAINTENANCE, dtype: object\n",
            "0      acc\n",
            "1    unacc\n",
            "2    unacc\n",
            "3    unacc\n",
            "4    unacc\n",
            "Name: CLASS, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Access the 'BUYING' column as a Series using square bracket notation\n",
        "buying_price = train_df['BUYING']\n",
        "print(buying_price.head())\n",
        "\n",
        "# Access the 'MAINTENANCE' column:\n",
        "maintenance_cost = train_df['MAINTENANCE']\n",
        "print(maintenance_cost.head())\n",
        "\n",
        "# Access the 'CLASS' column in the test dataset as a Series\n",
        "labels = train_df['CLASS']\n",
        "print(labels.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JfDTOKe3sTm"
      },
      "source": [
        "7. Use value_counts() function to  find the number of samples for each distinct value for a particular column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKLkuPXm46YG",
        "outputId": "7a6bd368-2f78-424c-a13e-4ed8c534fb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of each distinct value in 'BUYING':\n",
            "vhigh    355\n",
            "high     355\n",
            "low      350\n",
            "med      340\n",
            "Name: MAINTENANCE, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Counts of each distinct value in 'BUYING':\")\n",
        "print (maintenance_cost.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueIDC_tn_lLu"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Questions\n",
        "---\n",
        "\n",
        "## - Part 1: Check Terminal Node Condition:\n",
        "(Q.1., **5 Marks**): In the first step, we need to check if a node containing a DataFrame is a terminal node or it needs further splitting. Implement a function called \"check_if_terminal\" to do this task.\n",
        "\n",
        "Function Requirements:\n",
        "\n",
        "Input:\n",
        "\n",
        "\n",
        "*   parent_data: the DataFrame corresponding to a node.\n",
        "\n",
        "*   threshold: Proportion threshold for the majority class.\n",
        "\n",
        "\n",
        "\n",
        "Calculate the proportion of samples with the majority class label.\n",
        "\n",
        "If the proportion â‰¥ threshold, return \"Leaf\" as flag.\n",
        "\n",
        "If the proportion < threshold, return \"Internal\" as the flag.\n",
        "\n",
        "In addition to the flag, the function must return majority class (\"acc\"/\"unacc\"/\"good\", \"vgood\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkKqk351BFbr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_if_terminal(dataframe, threshold):\n",
        "  # Get all attribute names from the DataFrame\n",
        "  all_attrs = dataframe.columns\n",
        "\n",
        "  # Select the last attribute as the class attribute\n",
        "  class_attrs = all_attrs[-1]\n",
        "\n",
        "  # Extract the labels (values of the class attribute)\n",
        "  labels = dataframe[class_attrs]\n",
        "\n",
        "  #.................................\n",
        "  # write the rest here:\n",
        "\n",
        "  class_counts = labels.value_counts()\n",
        "  # print(f\"type of class_counts: {type(class_counts)}\")\n",
        "  # print(f\"\\n\\n dataframe:\\n{dataframe}\\n\\n\")\n",
        "  # print(f\"\\n\\n class_counts:\\n{class_counts}\\n\\n\")\n",
        "  # print(f\"\\n\\n most frequent class's count:\\n{class_counts.iloc[0]}\\n\\n\")\n",
        "  # print(f\"\\n\\n most frequent class:\\n{class_counts.iloc[:1].index[0]}\\n\\n\")\n",
        "  # print(f\"\\n\\n biggest proportion percent:\\n{class_counts.iloc[0]/ len(dataframe.index)}\\n\\n\")\n",
        "\n",
        "  majority_proportion = (class_counts.iloc[0]/ len(dataframe.index))\n",
        "  majority_class = class_counts.iloc[:1].index[0]\n",
        "\n",
        "  if float(majority_proportion) >= threshold:\n",
        "    flag = \"Leaf\"\n",
        "  else:\n",
        "    flag = \"Internal\"\n",
        "\n",
        "  # output flag must be a string (whether \"Internal\" or \"Leaf\")\n",
        "  # majority_class must be a string indicating the majority label of the samples in the node\n",
        "  #..................................\n",
        "  return flag, majority_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyHOFah3-1Oh",
        "outputId": "2075d50a-84ea-44b7-c520-f5efe59f1b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the node type is Internal\n",
            "the majority class of the node is unacc\n"
          ]
        }
      ],
      "source": [
        "# Check your implementation on training dataframe:\n",
        "flag, majority_class = check_if_terminal(train_df, 0.9)\n",
        "print(\"the node type is {}\".format(flag))\n",
        "print(\"the majority class of the node is {}\".format(majority_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTq3CiKq5SFS"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## - Part 2: Entropy Function:\n",
        "(Q.2.,  **10 Marks**): In order to split a node in a decision tree based on the Information Gain criterion, we need to calculate the entropy of the samples. Entropy is a measure of impurity in the data, and it is used to quantify the uncertainty associated with a set of class labels.\n",
        "\n",
        "\n",
        "**Task:** Write a Python function called \"entropy\" that takes a the CLASS column of the dataframe denoted as \"label\" and returns the entropy as the output.\n",
        "\n",
        "Function Signature: def entropy(labels: list) -> float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkZoOwiZ7DMz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def entropy(labels):\n",
        "  # Count the occurrences of each unique label\n",
        "  value_counts = labels.value_counts()\n",
        "  #.................................\n",
        "  # write the rest here:\n",
        "  entp = 0\n",
        "  classes = labels.value_counts()\n",
        "  total_count = len(labels.index)\n",
        "  # print(f\"\\n\\ntotal count is {total_count}\\n\\n\")\n",
        "\n",
        "\n",
        "  # print(f\"\\n\\n class_counts:\\n{classes}\\n\\n\")\n",
        "\n",
        "  for element_count in classes:\n",
        "    # print(f\"element is {element_count}\")\n",
        "    proportion = float(element_count/total_count)\n",
        "    entp -= (proportion* math.log(proportion, 2))\n",
        "\n",
        "  #.................................\n",
        "  #Return the calculated entropy\n",
        "  return entp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAj5YHcz_bWG",
        "outputId": "d179f39f-6cd4-4b10-8f1e-f5de007fa69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entropy of the node is 1.1790359988713874\n"
          ]
        }
      ],
      "source": [
        "# Check your implementation on training dataframe:\n",
        "labels = train_df[\"CLASS\"]\n",
        "entrp = entropy(labels)\n",
        "print(\"entropy of the node is {}\".format(entrp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIUqZqOl8fie"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## - Part 3: Calculating Information Gain:\n",
        "(Q.3., **15 Marks**): In this step, you are required to implement a function named \"information_gain\" that computes the information gain obtained by splitting samples denoted by 'CLASS' column referenced as 'labels' based on a specific attribute column denoted as 'x'. It should be noted that both 'labels' and 'x' are columns of a DataFrame. Please use the function written in \"Part 2\" for this part.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izpdQGkh-PaM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def information_gain(x, labels):\n",
        "  #Calculate the entropy of the parent node\n",
        "  parent_entropy = entropy(labels)\n",
        "  #.................................\n",
        "  # write the rest here:\n",
        "\n",
        "  # get all child nodes for the attribute column x\n",
        "  child_nodes = x.value_counts().index\n",
        "\n",
        "  # total child entropy\n",
        "  childs_entropy = 0\n",
        "  # for each chile node\n",
        "  for child_node in child_nodes:\n",
        "    # use a boolean mask to identify which x has child_node label\n",
        "    mask = x == child_node\n",
        "\n",
        "    # isolate the labels for current child_node\n",
        "    filtered = labels[mask]\n",
        "\n",
        "    # calculate weight of current child entropy for total child entropy\n",
        "    weight = len(filtered) / len(x)\n",
        "\n",
        "    # calculate the entropy for current child node and add it to the total sum multipled by its weight\n",
        "    childs_entropy += weight * entropy(filtered)\n",
        "\n",
        "  #Calculate the information gain by subtracting child entropy from parent entropy\n",
        "  info_gain = parent_entropy - childs_entropy\n",
        "  #.................................\n",
        "  return info_gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SbID8sz_uv4",
        "outputId": "85de2ea7-6d74-4848-bd46-7dc05dd70452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "information gain of the node in splitting over PERSONS attribute is 0.21326310194104847\n"
          ]
        }
      ],
      "source": [
        "# Check your implementation for training dataframe on \"PERSONS\" attribute:\n",
        "labels = train_df[\"CLASS\"]\n",
        "x = train_df[\"PERSONS\"]\n",
        "info_gain = information_gain(x,labels)\n",
        "print(\"information gain of the node in splitting over PERSONS attribute is {}\".format(info_gain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_C2rfRQPNxE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - Part 4: Selecting the Best Attribute for Splitting\n",
        "(Q.4., **10 Marks**): In this part, you are tasked with implementing a function called \"select_attribute.\" This function will take a parent DataFrame referenced as \"parent_data\" along with a list of splittable attributes denoted by \"remaining_attrs\" as the input and returns a string representing name of the best attribute which yields to the highest information gain after splitting. You may use the function written in \"Part 3\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrRSD7ezQyyH"
      },
      "outputs": [],
      "source": [
        "def select_attribute(parent_data, remaining_attrs):\n",
        "  all_attrs = parent_data.columns\n",
        "  # Extract the class attribute:\n",
        "  class_attr = all_attrs[-1]\n",
        "\n",
        "  # Extract the labels (target values) from the parent data\n",
        "  labels = parent_data[class_attr]\n",
        "\n",
        "  #.................................\n",
        "  # write the rest here:\n",
        "  # Loop through \"remaining_attrs\" attributes and calculate their information gains\n",
        "\n",
        "  dictionary = dict()\n",
        "\n",
        "  for attr in remaining_attrs:\n",
        "    info_gain = information_gain(parent_data[attr], labels)\n",
        "    dictionary[attr] = info_gain\n",
        "\n",
        "  # Find the attribute with the highest information gain and return it as sel_attr\n",
        "  #.................................\n",
        "\n",
        "  # get attribute name of max information gain\n",
        "  sel_attr = max(dictionary, key=dictionary.get)\n",
        "\n",
        "  return sel_attr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SPm6-CnAgfh",
        "outputId": "16861a5f-c93b-44dc-d42c-0c7856910190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best attribute for splitting the node is SAFETY\n"
          ]
        }
      ],
      "source": [
        "# Check your implementation on training dataframe:\n",
        "remaining_attrs = list(train_df.columns[:-1])\n",
        "sel_attr = select_attribute(train_df, remaining_attrs)\n",
        "print(\"the best attribute for splitting the node is {}\".format(sel_attr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UH1AkKUrCI8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "# - Part 5: Splitting the nodes at each tree level\n",
        "\n",
        "(Q.5., **20 Marks**):\n",
        "\n",
        "\n",
        " In this assignment, you will be implementing a crucial part of the decision tree implementation by creating a Python function called data_split. The purpose of this function is to split a parent node's dataframe into child dataframes based on the best attribute, which yields the highest information gain. You may use the helper functions that you have already implemented in previous sections.\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "* Write a function called \"data_split\" to split all the nodes in level \"n\" and to generate all the children nodes in level \"n+1\".\n",
        "\n",
        "* Perform node splitting in a systematic manner, progressing level by level. This entails creating all nodes at level n+1 by dividing all nodes eligible for splitting at level n. Refer to the example below for clarification:\n",
        "\n",
        "![Image](https://drive.google.com/uc?export=download&id=1kIOCkYaxUJMEKumBP6RxOLriQQY2Wlqx)\n",
        "  As depicted in the illustration, at level 1, there is a solitary node designated as \"Node_1_1,\" symbolizing the first node of the first level. Level 1 has been subdivided into three nodes, identified as \"Node_2_1,\" \"Node_2_2,\" and \"Node_2_3,\" signifying the first, second, and third nodes of the second level of splitting. Please adhere to this notation for naming each node.\n",
        "\n",
        "* Imagine a dictionary named \"dataframe_dict,\" where the \"keys\" correspond to the node names at a specific splitting level, and the \"values\" represent the associated dataframes. To illustrate, for level 1, the \"dataframe_dict\" would consist of a single key, \"Node_1_1,\" with the corresponding value being the primary dataframe:\n",
        "                dataframe_dict = {\"Node_1_1\": the main dataframe}\n",
        "In this example, following the execution of the \"data_split\" function, the \"dataframe_dict\" dictionary should be replaced with a dictionary containing three entries, as demonstrated below:\n",
        "      dataframe_dict = {\n",
        "                            \"Node_2_1\": dataframe_2_1,\n",
        "                            \"Node_2_2\": dataframe_2_2,\n",
        "                            \"Node_2_3\": dataframe_2_3\n",
        "                        }\n",
        "\n",
        "\n",
        "\n",
        "* Similarly, consider another dictionary called \"remaining_attrs\" with \"keys\" representing the nodes' names, and \"values\" representing the splittable attributes for each node.  For the first level, the \"remaining_attrs\" dictionary might be defined as:\n",
        "\n",
        "      remaining_attrs = {\n",
        "                            \"Node_1_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY']\n",
        "                        }\n",
        "but after running the function \"data_split\", it would be updated to a dictionary with three keys-values as:\n",
        "\n",
        "      remaining_attrs = {\n",
        "                         \"Node_2_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'],\n",
        "                         \"Node_2_2\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'] ,\n",
        "                         \"Node_2_3\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY']\n",
        "                         }\n",
        "\n",
        "Please note that once we've performed a split on a categorical attribute such as \"PERSONS\" and generated the children nodes in the subsequent level, we are no longer permitted to split on the same categorical attribute within that branch of the tree. It's important to emphasize that this restriction doesn't apply to numerical attributes.\n",
        "\n",
        "In the context of this example, this means that the \"remaining_attrs\" dictionary is updated to a three-element dictionary, where none of the nodes in this specific branch have the \"PERSONS\" attribute as a splittable option anymore.\n",
        "\n",
        "* Consider the \"tree_model\" as a list containing three additional dictionaries: \"tree_connectivity\", \"node_labels\", \"and node_types\":\n",
        "\n",
        "            tree_model = [tree_connectivity , node_types, node_labels]\n",
        "\n",
        "where \"tree_connectivity\" is a dictionary representing the node connection to the parents. The \"node_types\" and \"node_labels\" are also dictionaries containing the (\"Leaf\" or \"Internal\") and the majority class for each node, respectively. Your \"data_split\" function must take the \"tree_model\" generated up to  level \"n\" and must update it to the model up top level \"n+1\" after splitting. See the below image for this example:\n",
        "The tree_model at level 1 is:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1GSJzh4CNE298LFXQR86LYpEfj4Q883-S\"  width=400>\n",
        "\n",
        "\n",
        "After running the \"data_split\" function, the tree_model will be updated up to level 2 as follows:\n",
        "\n",
        "![Image](https://drive.google.com/uc?export=download&id=1Y3sGXHBpQtPVpMh8UnVlO0AYXvHNTGfo)\n",
        "\n",
        "\n",
        "**Therefore**: You must write the function \"data_split\" which takes \"dataframe_dict\", \"remaining_attrs\", \"tree_model\", \"level\", and \"threshold\" as the input and update \"dataframe_dict\", \"remaining_attrs\", and \"tree_model\" upto level \"level+1\". The function must also retun a boolean flag \"stop_train\" which must be True if any child node is generated. Otherwise, it must return False. Here, input \"threshold\" is the majority class threshold for checking wether a node is a \"Leaf\" node or an \"Internal\" node.\n",
        "\n",
        "**To complete the function**:\n",
        "* Loop through the nodes in \"dataframe_dict\" in the current \"level\". For each node, check if it's an \"Internal\" node and if so, find the best attribute for splitting. Create child nodes and finally update all the variables.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAsxLUcTrFtV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold):\n",
        "  # Unpack the tree_model list into three separate variables\n",
        "  [tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "  # Create an empty dictionary to store new child dataframes\n",
        "  dataframe_dict_new = {}\n",
        "  remaining_attrs_new = {}\n",
        "\n",
        "  # Initialize a counter for child nodes\n",
        "  child_ind = 1\n",
        "\n",
        "  #.................................\n",
        "  # write the rest here:\n",
        "\n",
        "  # Iterate over keys in the dataframe_dict (representing nodes at the current level)\n",
        "  for node in dataframe_dict:\n",
        "    if node_types[node] == \"Leaf\":\n",
        "      continue\n",
        "    df = dataframe_dict[node]\n",
        "    rmng_attr = remaining_attrs[node].copy()\n",
        "    attr = select_attribute(df, rmng_attr)\n",
        "\n",
        "    new_nodes = df[attr].unique()\n",
        "\n",
        "    tree_connectivity[node] = dict()\n",
        "\n",
        "    rmng_attr.remove(attr)\n",
        "\n",
        "    for new_node in new_nodes:\n",
        "\n",
        "      child_name = \"node_\" + str(level+1) + \"_\" + str(child_ind)\n",
        "\n",
        "      # add new remaining attributes to the remaining_attrs_new dictionary\n",
        "      remaining_attrs_new[child_name] = rmng_attr\n",
        "\n",
        "      # add new child node to the dataframe_dict_new dictionary\n",
        "      child_df = df[df[attr] == new_node]\n",
        "      dataframe_dict_new[child_name] = child_df\n",
        "\n",
        "      # add new child node to the tree_connectivity dictionary\n",
        "      tree_connectivity[node][f\"{attr}=={new_node}\"] = child_name\n",
        "\n",
        "      # majority class for label\n",
        "      node_labels[child_name] = check_if_terminal(child_df, threshold)[1]\n",
        "      node_types[child_name] = check_if_terminal(child_df, threshold)[0]\n",
        "      #node_types[child_name] = \"Leaf\"\n",
        "      # update child_ind\n",
        "      child_ind += 1\n",
        "\n",
        "\n",
        "\n",
        "  #replace the new old dictionaries with new dictionaries\n",
        "  dataframe_dict = dataframe_dict_new\n",
        "  remaining_attrs = remaining_attrs_new\n",
        "\n",
        "\n",
        "  # also return True as stop_train if no child node is generated. Otherwise return False\n",
        "  if dataframe_dict == {}:\n",
        "    stop_train = True\n",
        "  else:\n",
        "    stop_train = False\n",
        "\n",
        "\n",
        "#.................................\n",
        "  # Return the updated tree_model and a flag indicating whether training should stop\n",
        "  return tree_model, stop_train, dataframe_dict, remaining_attrs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbJDC_sFA5jm",
        "outputId": "04901d93-36ec-473f-c8ab-04b6b41686ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " tree connectivity:\n",
            "{'node_1_1': {'SAFETY==high': 'node_2_1', 'SAFETY==low': 'node_2_2', 'SAFETY==med': 'node_2_3'}}\n",
            "\n",
            " node labels:\n",
            "{'node_1_1': 'unacc', 'node_2_1': 'unacc', 'node_2_2': 'unacc', 'node_2_3': 'unacc'}\n",
            "\n",
            " node types:\n",
            "{'node_1_1': 'Internal', 'node_2_1': 'Internal', 'node_2_2': 'Leaf', 'node_2_3': 'Internal'}\n",
            "\n",
            " remaining attributes are:\n",
            "{'node_2_1': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT'], 'node_2_2': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT'], 'node_2_3': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT']}\n"
          ]
        }
      ],
      "source": [
        "# Now Check your implementation on training dataframe:\n",
        "# Initializing\n",
        "threshold = 0.9\n",
        "\n",
        "tree_connectivity = {}\n",
        "\n",
        "flag, majority_class = check_if_terminal(train_df, 0.9)\n",
        "\n",
        "node_types = {\"node_1_1\": flag}\n",
        "node_labels = {\"node_1_1\": majority_class}\n",
        "\n",
        "# Create an initial tree_model\n",
        "tree_model = [tree_connectivity, node_labels, node_types]\n",
        "\n",
        "# Create an initial dataframe_dict\n",
        "dataframe_dict = {\"node_1_1\": train_df}\n",
        "\n",
        "# Create an initial remaining_attrs\n",
        "\n",
        "independent_attrs = list(train_df.columns[:-1])\n",
        "remaining_attrs = {\"node_1_1\": independent_attrs}\n",
        "\n",
        "# Set level to 1\n",
        "level = 1\n",
        "\n",
        "\n",
        "# Update tree model\n",
        "tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold)\n",
        "\n",
        "\n",
        "[tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "print(\"\\n tree connectivity:\")\n",
        "print(tree_connectivity)\n",
        "\n",
        "print(\"\\n node labels:\")\n",
        "print(node_labels)\n",
        "\n",
        "print(\"\\n node types:\")\n",
        "print(node_types)\n",
        "\n",
        "print(\"\\n remaining attributes are:\")\n",
        "print(remaining_attrs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2ROrMYyUfR"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## -Part 6: Training the Decision Tree\n",
        "\n",
        "(Q.6., **10 Marks**): Now, let's create a function called \"tree_train\" to train the decision tree. This function begins by initializing the tree model and dataframe dictionary using the root node named \"node_1_1.\" It then iteratively updates these structures as it progresses through the tree, continuing until no further child nodes are generated. The process starts at level 1, and with each iteration, the level is incremented. Importantly, make sure to utilize the \"split_data\" function, which you've previously implemented, to assist in the tree construction. Ultimately, the function must return the fully trained tree model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EVuxiv2zD8c"
      },
      "outputs": [],
      "source": [
        "def tree_train(training_data, threshold):\n",
        "  # Initializing\n",
        "  tree_connectivity = {}\n",
        "\n",
        "  flag, majority_class = check_if_terminal(training_data, threshold)\n",
        "\n",
        "  node_types = {\"node_1_1\": flag}\n",
        "  node_labels = {\"node_1_1\": majority_class}\n",
        "\n",
        "  # Create a tree_model list to store connectivity, node labels, and node types\n",
        "  tree_model = [tree_connectivity, node_labels, node_types]\n",
        "\n",
        "  # Create a dataframe_dict with the initial training data and associate it with the root node\n",
        "  dataframe_dict = {\"node_1_1\": training_data}\n",
        "\n",
        "  # Create a remaining_attrs dictionary with all the independent attributes and associate it with the root node\n",
        "  indp_attrs = list(training_data.columns[:-1])\n",
        "  remaining_attrs = {\"node_1_1\": indp_attrs}\n",
        "\n",
        "  # Initialize the level of the tree to 1\n",
        "  level = 1\n",
        "\n",
        "  stop_train = False\n",
        "  # Continue tree construction until a stopping condition is met (use while loop)\n",
        "\n",
        "    #.................................\n",
        "  # write the rest here:\n",
        "  # write a loop function and exit the loop if terminating criterion is met\n",
        "  while not stop_train:\n",
        "    tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold)\n",
        "    level += 1\n",
        "\n",
        "    #.................................\n",
        "  # Return the final tree model\n",
        "  return tree_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHS5ytxLDEdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e049e1ce-951c-4cdb-ea62-42072ad21194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " tree connectivity:\n",
            "{'node_1_1': {'SAFETY==high': 'node_2_1', 'SAFETY==low': 'node_2_2', 'SAFETY==med': 'node_2_3'}, 'node_2_1': {'PERSONS==more': 'node_3_1', 'PERSONS==2': 'node_3_2', 'PERSONS==4': 'node_3_3'}, 'node_2_3': {'PERSONS==4': 'node_3_4', 'PERSONS==more': 'node_3_5', 'PERSONS==2': 'node_3_6'}, 'node_3_1': {'BUYING==vhigh': 'node_4_1', 'BUYING==low': 'node_4_2', 'BUYING==high': 'node_4_3', 'BUYING==med': 'node_4_4'}, 'node_3_3': {'BUYING==high': 'node_4_5', 'BUYING==low': 'node_4_6', 'BUYING==vhigh': 'node_4_7', 'BUYING==med': 'node_4_8'}, 'node_3_4': {'BUYING==vhigh': 'node_4_9', 'BUYING==low': 'node_4_10', 'BUYING==high': 'node_4_11', 'BUYING==med': 'node_4_12'}, 'node_3_5': {'BUYING==high': 'node_4_13', 'BUYING==low': 'node_4_14', 'BUYING==med': 'node_4_15', 'BUYING==vhigh': 'node_4_16'}, 'node_4_1': {'MAINTENANCE==low': 'node_5_1', 'MAINTENANCE==high': 'node_5_2', 'MAINTENANCE==vhigh': 'node_5_3', 'MAINTENANCE==med': 'node_5_4'}, 'node_4_2': {'MAINTENANCE==low': 'node_5_5', 'MAINTENANCE==high': 'node_5_6', 'MAINTENANCE==med': 'node_5_7', 'MAINTENANCE==vhigh': 'node_5_8'}, 'node_4_3': {'MAINTENANCE==vhigh': 'node_5_9', 'MAINTENANCE==high': 'node_5_10', 'MAINTENANCE==low': 'node_5_11', 'MAINTENANCE==med': 'node_5_12'}, 'node_4_4': {'MAINTENANCE==vhigh': 'node_5_13', 'MAINTENANCE==med': 'node_5_14', 'MAINTENANCE==high': 'node_5_15', 'MAINTENANCE==low': 'node_5_16'}, 'node_4_5': {'MAINTENANCE==low': 'node_5_17', 'MAINTENANCE==high': 'node_5_18', 'MAINTENANCE==vhigh': 'node_5_19', 'MAINTENANCE==med': 'node_5_20'}, 'node_4_6': {'MAINTENANCE==med': 'node_5_21', 'MAINTENANCE==high': 'node_5_22', 'MAINTENANCE==low': 'node_5_23', 'MAINTENANCE==vhigh': 'node_5_24'}, 'node_4_7': {'MAINTENANCE==low': 'node_5_25', 'MAINTENANCE==high': 'node_5_26', 'MAINTENANCE==med': 'node_5_27', 'MAINTENANCE==vhigh': 'node_5_28'}, 'node_4_8': {'MAINTENANCE==high': 'node_5_29', 'MAINTENANCE==med': 'node_5_30', 'MAINTENANCE==low': 'node_5_31', 'MAINTENANCE==vhigh': 'node_5_32'}, 'node_4_9': {'MAINTENANCE==med': 'node_5_33', 'MAINTENANCE==vhigh': 'node_5_34', 'MAINTENANCE==low': 'node_5_35', 'MAINTENANCE==high': 'node_5_36'}, 'node_4_10': {'MAINTENANCE==vhigh': 'node_5_37', 'MAINTENANCE==high': 'node_5_38', 'MAINTENANCE==low': 'node_5_39', 'MAINTENANCE==med': 'node_5_40'}, 'node_4_11': {'LUG_BOOT==small': 'node_5_41', 'LUG_BOOT==med': 'node_5_42', 'LUG_BOOT==big': 'node_5_43'}, 'node_4_12': {'MAINTENANCE==low': 'node_5_44', 'MAINTENANCE==vhigh': 'node_5_45', 'MAINTENANCE==med': 'node_5_46', 'MAINTENANCE==high': 'node_5_47'}, 'node_4_13': {'LUG_BOOT==med': 'node_5_48', 'LUG_BOOT==small': 'node_5_49', 'LUG_BOOT==big': 'node_5_50'}, 'node_4_14': {'MAINTENANCE==vhigh': 'node_5_51', 'MAINTENANCE==high': 'node_5_52', 'MAINTENANCE==low': 'node_5_53', 'MAINTENANCE==med': 'node_5_54'}, 'node_4_15': {'MAINTENANCE==low': 'node_5_55', 'MAINTENANCE==vhigh': 'node_5_56', 'MAINTENANCE==med': 'node_5_57', 'MAINTENANCE==high': 'node_5_58'}, 'node_4_16': {'MAINTENANCE==low': 'node_5_59', 'MAINTENANCE==vhigh': 'node_5_60', 'MAINTENANCE==high': 'node_5_61', 'MAINTENANCE==med': 'node_5_62'}, 'node_5_4': {'DOORS==3': 'node_6_1', 'DOORS==2': 'node_6_2', 'DOORS==4': 'node_6_3', 'DOORS==5more': 'node_6_4'}, 'node_5_5': {'LUG_BOOT==big': 'node_6_5', 'LUG_BOOT==med': 'node_6_6', 'LUG_BOOT==small': 'node_6_7'}, 'node_5_6': {'LUG_BOOT==small': 'node_6_8', 'LUG_BOOT==big': 'node_6_9', 'LUG_BOOT==med': 'node_6_10'}, 'node_5_7': {'LUG_BOOT==small': 'node_6_11', 'LUG_BOOT==big': 'node_6_12', 'LUG_BOOT==med': 'node_6_13'}, 'node_5_8': {'DOORS==4': 'node_6_14', 'DOORS==3': 'node_6_15', 'DOORS==2': 'node_6_16', 'DOORS==5more': 'node_6_17'}, 'node_5_10': {'DOORS==4': 'node_6_18', 'DOORS==2': 'node_6_19', 'DOORS==5more': 'node_6_20', 'DOORS==3': 'node_6_21'}, 'node_5_14': {'LUG_BOOT==small': 'node_6_22', 'LUG_BOOT==med': 'node_6_23', 'LUG_BOOT==big': 'node_6_24'}, 'node_5_15': {'DOORS==5more': 'node_6_25', 'DOORS==3': 'node_6_26', 'DOORS==4': 'node_6_27', 'DOORS==2': 'node_6_28'}, 'node_5_16': {'LUG_BOOT==med': 'node_6_29', 'LUG_BOOT==big': 'node_6_30', 'LUG_BOOT==small': 'node_6_31'}, 'node_5_21': {'LUG_BOOT==big': 'node_6_32', 'LUG_BOOT==small': 'node_6_33', 'LUG_BOOT==med': 'node_6_34'}, 'node_5_22': {'LUG_BOOT==big': 'node_6_35', 'LUG_BOOT==small': 'node_6_36', 'LUG_BOOT==med': 'node_6_37'}, 'node_5_23': {'LUG_BOOT==big': 'node_6_38', 'LUG_BOOT==small': 'node_6_39', 'LUG_BOOT==med': 'node_6_40'}, 'node_5_30': {'LUG_BOOT==med': 'node_6_41', 'LUG_BOOT==big': 'node_6_42', 'LUG_BOOT==small': 'node_6_43'}, 'node_5_31': {'LUG_BOOT==med': 'node_6_44', 'LUG_BOOT==small': 'node_6_45', 'LUG_BOOT==big': 'node_6_46'}, 'node_5_33': {'LUG_BOOT==med': 'node_6_47', 'LUG_BOOT==small': 'node_6_48', 'LUG_BOOT==big': 'node_6_49'}, 'node_5_35': {'LUG_BOOT==med': 'node_6_50', 'LUG_BOOT==small': 'node_6_51', 'LUG_BOOT==big': 'node_6_52'}, 'node_5_37': {'LUG_BOOT==big': 'node_6_53', 'LUG_BOOT==med': 'node_6_54', 'LUG_BOOT==small': 'node_6_55'}, 'node_5_39': {'LUG_BOOT==med': 'node_6_56', 'LUG_BOOT==big': 'node_6_57', 'LUG_BOOT==small': 'node_6_58'}, 'node_5_40': {'LUG_BOOT==med': 'node_6_59', 'LUG_BOOT==small': 'node_6_60', 'LUG_BOOT==big': 'node_6_61'}, 'node_5_42': {'DOORS==5more': 'node_6_62', 'DOORS==3': 'node_6_63', 'DOORS==2': 'node_6_64', 'DOORS==4': 'node_6_65'}, 'node_5_43': {'MAINTENANCE==low': 'node_6_66', 'MAINTENANCE==vhigh': 'node_6_67', 'MAINTENANCE==med': 'node_6_68', 'MAINTENANCE==high': 'node_6_69'}, 'node_5_44': {'LUG_BOOT==big': 'node_6_70', 'LUG_BOOT==med': 'node_6_71', 'LUG_BOOT==small': 'node_6_72'}, 'node_5_45': {'LUG_BOOT==med': 'node_6_73', 'LUG_BOOT==small': 'node_6_74', 'LUG_BOOT==big': 'node_6_75'}, 'node_5_47': {'LUG_BOOT==small': 'node_6_76', 'LUG_BOOT==big': 'node_6_77', 'LUG_BOOT==med': 'node_6_78'}, 'node_5_48': {'MAINTENANCE==med': 'node_6_79', 'MAINTENANCE==high': 'node_6_80', 'MAINTENANCE==low': 'node_6_81', 'MAINTENANCE==vhigh': 'node_6_82'}, 'node_5_50': {'MAINTENANCE==vhigh': 'node_6_83', 'MAINTENANCE==med': 'node_6_84', 'MAINTENANCE==high': 'node_6_85', 'MAINTENANCE==low': 'node_6_86'}, 'node_5_51': {'LUG_BOOT==med': 'node_6_87', 'LUG_BOOT==big': 'node_6_88', 'LUG_BOOT==small': 'node_6_89'}, 'node_5_53': {'LUG_BOOT==big': 'node_6_90', 'LUG_BOOT==small': 'node_6_91', 'LUG_BOOT==med': 'node_6_92'}, 'node_5_54': {'LUG_BOOT==big': 'node_6_93', 'LUG_BOOT==med': 'node_6_94', 'LUG_BOOT==small': 'node_6_95'}, 'node_5_55': {'LUG_BOOT==big': 'node_6_96', 'LUG_BOOT==med': 'node_6_97', 'LUG_BOOT==small': 'node_6_98'}, 'node_5_56': {'LUG_BOOT==big': 'node_6_99', 'LUG_BOOT==med': 'node_6_100', 'LUG_BOOT==small': 'node_6_101'}, 'node_5_58': {'LUG_BOOT==big': 'node_6_102', 'LUG_BOOT==small': 'node_6_103', 'LUG_BOOT==med': 'node_6_104'}, 'node_5_59': {'LUG_BOOT==small': 'node_6_105', 'LUG_BOOT==med': 'node_6_106', 'LUG_BOOT==big': 'node_6_107'}, 'node_5_62': {'LUG_BOOT==small': 'node_6_108', 'LUG_BOOT==med': 'node_6_109', 'LUG_BOOT==big': 'node_6_110'}, 'node_6_2': {'LUG_BOOT==med': 'node_7_1', 'LUG_BOOT==small': 'node_7_2', 'LUG_BOOT==big': 'node_7_3'}, 'node_6_6': {'DOORS==4': 'node_7_4', 'DOORS==2': 'node_7_5', 'DOORS==3': 'node_7_6', 'DOORS==5more': 'node_7_7'}, 'node_6_8': {'DOORS==3': 'node_7_8', 'DOORS==4': 'node_7_9', 'DOORS==5more': 'node_7_10', 'DOORS==2': 'node_7_11'}, 'node_6_11': {'DOORS==3': 'node_7_12', 'DOORS==2': 'node_7_13', 'DOORS==4': 'node_7_14'}, 'node_6_16': {'LUG_BOOT==med': 'node_7_15', 'LUG_BOOT==small': 'node_7_16'}, 'node_6_19': {'LUG_BOOT==small': 'node_7_17', 'LUG_BOOT==big': 'node_7_18'}, 'node_6_22': {'DOORS==5more': 'node_7_19', 'DOORS==4': 'node_7_20', 'DOORS==2': 'node_7_21', 'DOORS==3': 'node_7_22'}, 'node_6_23': {'DOORS==3': 'node_7_23', 'DOORS==5more': 'node_7_24', 'DOORS==4': 'node_7_25', 'DOORS==2': 'node_7_26'}, 'node_6_28': {'LUG_BOOT==small': 'node_7_27', 'LUG_BOOT==med': 'node_7_28'}, 'node_6_29': {'DOORS==2': 'node_7_29', 'DOORS==4': 'node_7_30', 'DOORS==3': 'node_7_31'}, 'node_6_31': {'DOORS==2': 'node_7_32', 'DOORS==4': 'node_7_33', 'DOORS==3': 'node_7_34', 'DOORS==5more': 'node_7_35'}, 'node_6_34': {'DOORS==4': 'node_7_36', 'DOORS==2': 'node_7_37', 'DOORS==5more': 'node_7_38', 'DOORS==3': 'node_7_39'}, 'node_6_37': {'DOORS==2': 'node_7_40', 'DOORS==4': 'node_7_41', 'DOORS==5more': 'node_7_42'}, 'node_6_40': {'DOORS==4': 'node_7_43', 'DOORS==3': 'node_7_44', 'DOORS==2': 'node_7_45'}, 'node_6_41': {'DOORS==5more': 'node_7_46', 'DOORS==4': 'node_7_47', 'DOORS==2': 'node_7_48', 'DOORS==3': 'node_7_49'}, 'node_6_44': {'DOORS==4': 'node_7_50', 'DOORS==2': 'node_7_51', 'DOORS==5more': 'node_7_52'}, 'node_6_47': {'DOORS==3': 'node_7_53', 'DOORS==4': 'node_7_54', 'DOORS==5more': 'node_7_55'}, 'node_6_50': {'DOORS==4': 'node_7_56', 'DOORS==2': 'node_7_57', 'DOORS==5more': 'node_7_58', 'DOORS==3': 'node_7_59'}, 'node_6_54': {'DOORS==4': 'node_7_60', 'DOORS==2': 'node_7_61', 'DOORS==5more': 'node_7_62'}, 'node_6_56': {'DOORS==2': 'node_7_63', 'DOORS==4': 'node_7_64', 'DOORS==5more': 'node_7_65', 'DOORS==3': 'node_7_66'}, 'node_6_59': {'DOORS==3': 'node_7_67', 'DOORS==2': 'node_7_68', 'DOORS==5more': 'node_7_69'}, 'node_6_62': {'MAINTENANCE==med': 'node_7_70', 'MAINTENANCE==low': 'node_7_71', 'MAINTENANCE==vhigh': 'node_7_72', 'MAINTENANCE==high': 'node_7_73'}, 'node_6_65': {'MAINTENANCE==low': 'node_7_74', 'MAINTENANCE==vhigh': 'node_7_75', 'MAINTENANCE==high': 'node_7_76', 'MAINTENANCE==med': 'node_7_77'}, 'node_6_71': {'DOORS==2': 'node_7_78', 'DOORS==3': 'node_7_79', 'DOORS==5more': 'node_7_80', 'DOORS==4': 'node_7_81'}, 'node_6_73': {'DOORS==2': 'node_7_82', 'DOORS==4': 'node_7_83', 'DOORS==5more': 'node_7_84', 'DOORS==3': 'node_7_85'}, 'node_6_79': {'DOORS==2': 'node_7_86', 'DOORS==3': 'node_7_87', 'DOORS==5more': 'node_7_88', 'DOORS==4': 'node_7_89'}, 'node_6_80': {'DOORS==2': 'node_7_90', 'DOORS==3': 'node_7_91', 'DOORS==5more': 'node_7_92', 'DOORS==4': 'node_7_93'}, 'node_6_87': {'DOORS==2': 'node_7_94', 'DOORS==5more': 'node_7_95', 'DOORS==4': 'node_7_96', 'DOORS==3': 'node_7_97'}, 'node_6_94': {'DOORS==5more': 'node_7_98', 'DOORS==2': 'node_7_99', 'DOORS==4': 'node_7_100'}, 'node_6_95': {'DOORS==2': 'node_7_101', 'DOORS==5more': 'node_7_102'}, 'node_6_98': {'DOORS==3': 'node_7_103', 'DOORS==5more': 'node_7_104', 'DOORS==2': 'node_7_105'}, 'node_6_100': {'DOORS==2': 'node_7_106', 'DOORS==3': 'node_7_107', 'DOORS==5more': 'node_7_108', 'DOORS==4': 'node_7_109'}, 'node_6_104': {'DOORS==2': 'node_7_110', 'DOORS==5more': 'node_7_111'}, 'node_6_106': {'DOORS==2': 'node_7_112', 'DOORS==4': 'node_7_113', 'DOORS==3': 'node_7_114'}, 'node_6_109': {'DOORS==5more': 'node_7_115', 'DOORS==2': 'node_7_116', 'DOORS==4': 'node_7_117', 'DOORS==3': 'node_7_118'}}\n",
            "\n",
            " node labels:\n",
            "{'node_1_1': 'unacc', 'node_2_1': 'unacc', 'node_2_2': 'unacc', 'node_2_3': 'unacc', 'node_3_1': 'acc', 'node_3_2': 'unacc', 'node_3_3': 'acc', 'node_3_4': 'unacc', 'node_3_5': 'acc', 'node_3_6': 'unacc', 'node_4_1': 'unacc', 'node_4_2': 'vgood', 'node_4_3': 'acc', 'node_4_4': 'acc', 'node_4_5': 'acc', 'node_4_6': 'vgood', 'node_4_7': 'acc', 'node_4_8': 'acc', 'node_4_9': 'unacc', 'node_4_10': 'acc', 'node_4_11': 'unacc', 'node_4_12': 'acc', 'node_4_13': 'unacc', 'node_4_14': 'acc', 'node_4_15': 'acc', 'node_4_16': 'unacc', 'node_5_1': 'acc', 'node_5_2': 'unacc', 'node_5_3': 'unacc', 'node_5_4': 'acc', 'node_5_5': 'vgood', 'node_5_6': 'vgood', 'node_5_7': 'vgood', 'node_5_8': 'acc', 'node_5_9': 'unacc', 'node_5_10': 'acc', 'node_5_11': 'acc', 'node_5_12': 'acc', 'node_5_13': 'acc', 'node_5_14': 'vgood', 'node_5_15': 'acc', 'node_5_16': 'vgood', 'node_5_17': 'acc', 'node_5_18': 'acc', 'node_5_19': 'unacc', 'node_5_20': 'acc', 'node_5_21': 'vgood', 'node_5_22': 'vgood', 'node_5_23': 'good', 'node_5_24': 'acc', 'node_5_25': 'acc', 'node_5_26': 'unacc', 'node_5_27': 'acc', 'node_5_28': 'unacc', 'node_5_29': 'acc', 'node_5_30': 'acc', 'node_5_31': 'vgood', 'node_5_32': 'acc', 'node_5_33': 'unacc', 'node_5_34': 'unacc', 'node_5_35': 'unacc', 'node_5_36': 'unacc', 'node_5_37': 'acc', 'node_5_38': 'acc', 'node_5_39': 'good', 'node_5_40': 'acc', 'node_5_41': 'unacc', 'node_5_42': 'unacc', 'node_5_43': 'acc', 'node_5_44': 'good', 'node_5_45': 'unacc', 'node_5_46': 'acc', 'node_5_47': 'acc', 'node_5_48': 'acc', 'node_5_49': 'unacc', 'node_5_50': 'acc', 'node_5_51': 'acc', 'node_5_52': 'acc', 'node_5_53': 'good', 'node_5_54': 'good', 'node_5_55': 'good', 'node_5_56': 'acc', 'node_5_57': 'acc', 'node_5_58': 'acc', 'node_5_59': 'unacc', 'node_5_60': 'unacc', 'node_5_61': 'unacc', 'node_5_62': 'acc', 'node_6_1': 'acc', 'node_6_2': 'acc', 'node_6_3': 'acc', 'node_6_4': 'acc', 'node_6_5': 'vgood', 'node_6_6': 'vgood', 'node_6_7': 'good', 'node_6_8': 'acc', 'node_6_9': 'vgood', 'node_6_10': 'vgood', 'node_6_11': 'good', 'node_6_12': 'vgood', 'node_6_13': 'vgood', 'node_6_14': 'acc', 'node_6_15': 'acc', 'node_6_16': 'acc', 'node_6_17': 'acc', 'node_6_18': 'acc', 'node_6_19': 'unacc', 'node_6_20': 'acc', 'node_6_21': 'acc', 'node_6_22': 'acc', 'node_6_23': 'vgood', 'node_6_24': 'vgood', 'node_6_25': 'acc', 'node_6_26': 'acc', 'node_6_27': 'acc', 'node_6_28': 'unacc', 'node_6_29': 'vgood', 'node_6_30': 'vgood', 'node_6_31': 'good', 'node_6_32': 'vgood', 'node_6_33': 'good', 'node_6_34': 'vgood', 'node_6_35': 'vgood', 'node_6_36': 'acc', 'node_6_37': 'vgood', 'node_6_38': 'vgood', 'node_6_39': 'good', 'node_6_40': 'good', 'node_6_41': 'vgood', 'node_6_42': 'vgood', 'node_6_43': 'acc', 'node_6_44': 'vgood', 'node_6_45': 'good', 'node_6_46': 'vgood', 'node_6_47': 'acc', 'node_6_48': 'unacc', 'node_6_49': 'acc', 'node_6_50': 'acc', 'node_6_51': 'unacc', 'node_6_52': 'acc', 'node_6_53': 'acc', 'node_6_54': 'acc', 'node_6_55': 'unacc', 'node_6_56': 'acc', 'node_6_57': 'good', 'node_6_58': 'acc', 'node_6_59': 'acc', 'node_6_60': 'acc', 'node_6_61': 'good', 'node_6_62': 'acc', 'node_6_63': 'unacc', 'node_6_64': 'unacc', 'node_6_65': 'acc', 'node_6_66': 'acc', 'node_6_67': 'unacc', 'node_6_68': 'acc', 'node_6_69': 'acc', 'node_6_70': 'good', 'node_6_71': 'acc', 'node_6_72': 'acc', 'node_6_73': 'unacc', 'node_6_74': 'unacc', 'node_6_75': 'acc', 'node_6_76': 'unacc', 'node_6_77': 'acc', 'node_6_78': 'acc', 'node_6_79': 'acc', 'node_6_80': 'acc', 'node_6_81': 'acc', 'node_6_82': 'unacc', 'node_6_83': 'unacc', 'node_6_84': 'acc', 'node_6_85': 'acc', 'node_6_86': 'acc', 'node_6_87': 'acc', 'node_6_88': 'acc', 'node_6_89': 'unacc', 'node_6_90': 'good', 'node_6_91': 'acc', 'node_6_92': 'good', 'node_6_93': 'good', 'node_6_94': 'good', 'node_6_95': 'unacc', 'node_6_96': 'good', 'node_6_97': 'good', 'node_6_98': 'acc', 'node_6_99': 'acc', 'node_6_100': 'acc', 'node_6_101': 'unacc', 'node_6_102': 'acc', 'node_6_103': 'unacc', 'node_6_104': 'unacc', 'node_6_105': 'unacc', 'node_6_106': 'acc', 'node_6_107': 'acc', 'node_6_108': 'unacc', 'node_6_109': 'acc', 'node_6_110': 'acc', 'node_7_1': 'acc', 'node_7_2': 'unacc', 'node_7_3': 'acc', 'node_7_4': 'vgood', 'node_7_5': 'good', 'node_7_6': 'vgood', 'node_7_7': 'vgood', 'node_7_8': 'acc', 'node_7_9': 'acc', 'node_7_10': 'acc', 'node_7_11': 'unacc', 'node_7_12': 'good', 'node_7_13': 'unacc', 'node_7_14': 'good', 'node_7_15': 'acc', 'node_7_16': 'unacc', 'node_7_17': 'unacc', 'node_7_18': 'acc', 'node_7_19': 'acc', 'node_7_20': 'acc', 'node_7_21': 'unacc', 'node_7_22': 'acc', 'node_7_23': 'vgood', 'node_7_24': 'vgood', 'node_7_25': 'vgood', 'node_7_26': 'acc', 'node_7_27': 'unacc', 'node_7_28': 'acc', 'node_7_29': 'good', 'node_7_30': 'vgood', 'node_7_31': 'vgood', 'node_7_32': 'unacc', 'node_7_33': 'good', 'node_7_34': 'good', 'node_7_35': 'good', 'node_7_36': 'vgood', 'node_7_37': 'good', 'node_7_38': 'vgood', 'node_7_39': 'good', 'node_7_40': 'acc', 'node_7_41': 'vgood', 'node_7_42': 'vgood', 'node_7_43': 'vgood', 'node_7_44': 'good', 'node_7_45': 'good', 'node_7_46': 'vgood', 'node_7_47': 'vgood', 'node_7_48': 'acc', 'node_7_49': 'acc', 'node_7_50': 'vgood', 'node_7_51': 'good', 'node_7_52': 'vgood', 'node_7_53': 'unacc', 'node_7_54': 'acc', 'node_7_55': 'acc', 'node_7_56': 'acc', 'node_7_57': 'unacc', 'node_7_58': 'acc', 'node_7_59': 'unacc', 'node_7_60': 'acc', 'node_7_61': 'unacc', 'node_7_62': 'acc', 'node_7_63': 'acc', 'node_7_64': 'good', 'node_7_65': 'good', 'node_7_66': 'acc', 'node_7_67': 'acc', 'node_7_68': 'acc', 'node_7_69': 'good', 'node_7_70': 'acc', 'node_7_71': 'acc', 'node_7_72': 'unacc', 'node_7_73': 'acc', 'node_7_74': 'acc', 'node_7_75': 'unacc', 'node_7_76': 'acc', 'node_7_77': 'acc', 'node_7_78': 'acc', 'node_7_79': 'acc', 'node_7_80': 'good', 'node_7_81': 'good', 'node_7_82': 'unacc', 'node_7_83': 'acc', 'node_7_84': 'acc', 'node_7_85': 'unacc', 'node_7_86': 'unacc', 'node_7_87': 'acc', 'node_7_88': 'acc', 'node_7_89': 'acc', 'node_7_90': 'unacc', 'node_7_91': 'acc', 'node_7_92': 'acc', 'node_7_93': 'acc', 'node_7_94': 'unacc', 'node_7_95': 'acc', 'node_7_96': 'acc', 'node_7_97': 'acc', 'node_7_98': 'good', 'node_7_99': 'acc', 'node_7_100': 'good', 'node_7_101': 'unacc', 'node_7_102': 'acc', 'node_7_103': 'acc', 'node_7_104': 'acc', 'node_7_105': 'unacc', 'node_7_106': 'unacc', 'node_7_107': 'acc', 'node_7_108': 'acc', 'node_7_109': 'acc', 'node_7_110': 'unacc', 'node_7_111': 'acc', 'node_7_112': 'unacc', 'node_7_113': 'acc', 'node_7_114': 'acc', 'node_7_115': 'acc', 'node_7_116': 'unacc', 'node_7_117': 'acc', 'node_7_118': 'acc'}\n",
            "\n",
            " node types:\n",
            "{'node_1_1': 'Internal', 'node_2_1': 'Internal', 'node_2_2': 'Leaf', 'node_2_3': 'Internal', 'node_3_1': 'Internal', 'node_3_2': 'Leaf', 'node_3_3': 'Internal', 'node_3_4': 'Internal', 'node_3_5': 'Internal', 'node_3_6': 'Leaf', 'node_4_1': 'Internal', 'node_4_2': 'Internal', 'node_4_3': 'Internal', 'node_4_4': 'Internal', 'node_4_5': 'Internal', 'node_4_6': 'Internal', 'node_4_7': 'Internal', 'node_4_8': 'Internal', 'node_4_9': 'Internal', 'node_4_10': 'Internal', 'node_4_11': 'Internal', 'node_4_12': 'Internal', 'node_4_13': 'Internal', 'node_4_14': 'Internal', 'node_4_15': 'Internal', 'node_4_16': 'Internal', 'node_5_1': 'Leaf', 'node_5_2': 'Leaf', 'node_5_3': 'Leaf', 'node_5_4': 'Internal', 'node_5_5': 'Internal', 'node_5_6': 'Internal', 'node_5_7': 'Internal', 'node_5_8': 'Internal', 'node_5_9': 'Leaf', 'node_5_10': 'Internal', 'node_5_11': 'Leaf', 'node_5_12': 'Leaf', 'node_5_13': 'Leaf', 'node_5_14': 'Internal', 'node_5_15': 'Internal', 'node_5_16': 'Internal', 'node_5_17': 'Leaf', 'node_5_18': 'Leaf', 'node_5_19': 'Leaf', 'node_5_20': 'Leaf', 'node_5_21': 'Internal', 'node_5_22': 'Internal', 'node_5_23': 'Internal', 'node_5_24': 'Leaf', 'node_5_25': 'Leaf', 'node_5_26': 'Leaf', 'node_5_27': 'Leaf', 'node_5_28': 'Leaf', 'node_5_29': 'Leaf', 'node_5_30': 'Internal', 'node_5_31': 'Internal', 'node_5_32': 'Leaf', 'node_5_33': 'Internal', 'node_5_34': 'Leaf', 'node_5_35': 'Internal', 'node_5_36': 'Leaf', 'node_5_37': 'Internal', 'node_5_38': 'Leaf', 'node_5_39': 'Internal', 'node_5_40': 'Internal', 'node_5_41': 'Leaf', 'node_5_42': 'Internal', 'node_5_43': 'Internal', 'node_5_44': 'Internal', 'node_5_45': 'Internal', 'node_5_46': 'Leaf', 'node_5_47': 'Internal', 'node_5_48': 'Internal', 'node_5_49': 'Leaf', 'node_5_50': 'Internal', 'node_5_51': 'Internal', 'node_5_52': 'Leaf', 'node_5_53': 'Internal', 'node_5_54': 'Internal', 'node_5_55': 'Internal', 'node_5_56': 'Internal', 'node_5_57': 'Leaf', 'node_5_58': 'Internal', 'node_5_59': 'Internal', 'node_5_60': 'Leaf', 'node_5_61': 'Leaf', 'node_5_62': 'Internal', 'node_6_1': 'Leaf', 'node_6_2': 'Internal', 'node_6_3': 'Leaf', 'node_6_4': 'Leaf', 'node_6_5': 'Leaf', 'node_6_6': 'Internal', 'node_6_7': 'Leaf', 'node_6_8': 'Internal', 'node_6_9': 'Leaf', 'node_6_10': 'Leaf', 'node_6_11': 'Internal', 'node_6_12': 'Leaf', 'node_6_13': 'Leaf', 'node_6_14': 'Leaf', 'node_6_15': 'Leaf', 'node_6_16': 'Internal', 'node_6_17': 'Leaf', 'node_6_18': 'Leaf', 'node_6_19': 'Internal', 'node_6_20': 'Leaf', 'node_6_21': 'Leaf', 'node_6_22': 'Internal', 'node_6_23': 'Internal', 'node_6_24': 'Leaf', 'node_6_25': 'Leaf', 'node_6_26': 'Leaf', 'node_6_27': 'Leaf', 'node_6_28': 'Internal', 'node_6_29': 'Internal', 'node_6_30': 'Leaf', 'node_6_31': 'Internal', 'node_6_32': 'Leaf', 'node_6_33': 'Leaf', 'node_6_34': 'Internal', 'node_6_35': 'Leaf', 'node_6_36': 'Leaf', 'node_6_37': 'Internal', 'node_6_38': 'Leaf', 'node_6_39': 'Leaf', 'node_6_40': 'Internal', 'node_6_41': 'Internal', 'node_6_42': 'Leaf', 'node_6_43': 'Leaf', 'node_6_44': 'Internal', 'node_6_45': 'Leaf', 'node_6_46': 'Leaf', 'node_6_47': 'Internal', 'node_6_48': 'Leaf', 'node_6_49': 'Leaf', 'node_6_50': 'Internal', 'node_6_51': 'Leaf', 'node_6_52': 'Leaf', 'node_6_53': 'Leaf', 'node_6_54': 'Internal', 'node_6_55': 'Leaf', 'node_6_56': 'Internal', 'node_6_57': 'Leaf', 'node_6_58': 'Leaf', 'node_6_59': 'Internal', 'node_6_60': 'Leaf', 'node_6_61': 'Leaf', 'node_6_62': 'Internal', 'node_6_63': 'Leaf', 'node_6_64': 'Leaf', 'node_6_65': 'Internal', 'node_6_66': 'Leaf', 'node_6_67': 'Leaf', 'node_6_68': 'Leaf', 'node_6_69': 'Leaf', 'node_6_70': 'Leaf', 'node_6_71': 'Internal', 'node_6_72': 'Leaf', 'node_6_73': 'Internal', 'node_6_74': 'Leaf', 'node_6_75': 'Leaf', 'node_6_76': 'Leaf', 'node_6_77': 'Leaf', 'node_6_78': 'Leaf', 'node_6_79': 'Internal', 'node_6_80': 'Internal', 'node_6_81': 'Leaf', 'node_6_82': 'Leaf', 'node_6_83': 'Leaf', 'node_6_84': 'Leaf', 'node_6_85': 'Leaf', 'node_6_86': 'Leaf', 'node_6_87': 'Internal', 'node_6_88': 'Leaf', 'node_6_89': 'Leaf', 'node_6_90': 'Leaf', 'node_6_91': 'Leaf', 'node_6_92': 'Leaf', 'node_6_93': 'Leaf', 'node_6_94': 'Internal', 'node_6_95': 'Internal', 'node_6_96': 'Leaf', 'node_6_97': 'Leaf', 'node_6_98': 'Internal', 'node_6_99': 'Leaf', 'node_6_100': 'Internal', 'node_6_101': 'Leaf', 'node_6_102': 'Leaf', 'node_6_103': 'Leaf', 'node_6_104': 'Internal', 'node_6_105': 'Leaf', 'node_6_106': 'Internal', 'node_6_107': 'Leaf', 'node_6_108': 'Leaf', 'node_6_109': 'Internal', 'node_6_110': 'Leaf', 'node_7_1': 'Leaf', 'node_7_2': 'Leaf', 'node_7_3': 'Leaf', 'node_7_4': 'Leaf', 'node_7_5': 'Leaf', 'node_7_6': 'Leaf', 'node_7_7': 'Leaf', 'node_7_8': 'Leaf', 'node_7_9': 'Leaf', 'node_7_10': 'Leaf', 'node_7_11': 'Leaf', 'node_7_12': 'Leaf', 'node_7_13': 'Leaf', 'node_7_14': 'Leaf', 'node_7_15': 'Leaf', 'node_7_16': 'Leaf', 'node_7_17': 'Leaf', 'node_7_18': 'Leaf', 'node_7_19': 'Leaf', 'node_7_20': 'Leaf', 'node_7_21': 'Leaf', 'node_7_22': 'Leaf', 'node_7_23': 'Leaf', 'node_7_24': 'Leaf', 'node_7_25': 'Leaf', 'node_7_26': 'Leaf', 'node_7_27': 'Leaf', 'node_7_28': 'Leaf', 'node_7_29': 'Leaf', 'node_7_30': 'Leaf', 'node_7_31': 'Leaf', 'node_7_32': 'Leaf', 'node_7_33': 'Leaf', 'node_7_34': 'Leaf', 'node_7_35': 'Leaf', 'node_7_36': 'Leaf', 'node_7_37': 'Leaf', 'node_7_38': 'Leaf', 'node_7_39': 'Leaf', 'node_7_40': 'Leaf', 'node_7_41': 'Leaf', 'node_7_42': 'Leaf', 'node_7_43': 'Leaf', 'node_7_44': 'Leaf', 'node_7_45': 'Leaf', 'node_7_46': 'Leaf', 'node_7_47': 'Leaf', 'node_7_48': 'Leaf', 'node_7_49': 'Leaf', 'node_7_50': 'Leaf', 'node_7_51': 'Leaf', 'node_7_52': 'Leaf', 'node_7_53': 'Leaf', 'node_7_54': 'Leaf', 'node_7_55': 'Leaf', 'node_7_56': 'Leaf', 'node_7_57': 'Leaf', 'node_7_58': 'Leaf', 'node_7_59': 'Leaf', 'node_7_60': 'Leaf', 'node_7_61': 'Leaf', 'node_7_62': 'Leaf', 'node_7_63': 'Leaf', 'node_7_64': 'Leaf', 'node_7_65': 'Leaf', 'node_7_66': 'Leaf', 'node_7_67': 'Leaf', 'node_7_68': 'Leaf', 'node_7_69': 'Leaf', 'node_7_70': 'Leaf', 'node_7_71': 'Leaf', 'node_7_72': 'Leaf', 'node_7_73': 'Leaf', 'node_7_74': 'Leaf', 'node_7_75': 'Leaf', 'node_7_76': 'Leaf', 'node_7_77': 'Leaf', 'node_7_78': 'Leaf', 'node_7_79': 'Leaf', 'node_7_80': 'Leaf', 'node_7_81': 'Leaf', 'node_7_82': 'Leaf', 'node_7_83': 'Leaf', 'node_7_84': 'Leaf', 'node_7_85': 'Leaf', 'node_7_86': 'Leaf', 'node_7_87': 'Leaf', 'node_7_88': 'Leaf', 'node_7_89': 'Leaf', 'node_7_90': 'Leaf', 'node_7_91': 'Leaf', 'node_7_92': 'Leaf', 'node_7_93': 'Leaf', 'node_7_94': 'Leaf', 'node_7_95': 'Leaf', 'node_7_96': 'Leaf', 'node_7_97': 'Leaf', 'node_7_98': 'Leaf', 'node_7_99': 'Leaf', 'node_7_100': 'Leaf', 'node_7_101': 'Leaf', 'node_7_102': 'Leaf', 'node_7_103': 'Leaf', 'node_7_104': 'Leaf', 'node_7_105': 'Leaf', 'node_7_106': 'Leaf', 'node_7_107': 'Leaf', 'node_7_108': 'Leaf', 'node_7_109': 'Leaf', 'node_7_110': 'Leaf', 'node_7_111': 'Leaf', 'node_7_112': 'Leaf', 'node_7_113': 'Leaf', 'node_7_114': 'Leaf', 'node_7_115': 'Leaf', 'node_7_116': 'Leaf', 'node_7_117': 'Leaf', 'node_7_118': 'Leaf'}\n"
          ]
        }
      ],
      "source": [
        "# Check your implementation on training dataframe:\n",
        "tree_model = tree_train(train_df, 0.9)\n",
        "[tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "print(\"\\n tree connectivity:\")\n",
        "print(tree_connectivity)\n",
        "\n",
        "print(\"\\n node labels:\")\n",
        "print(node_labels)\n",
        "\n",
        "print(\"\\n node types:\")\n",
        "print(node_types)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty1X5YsJ25UT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Part 7: Prediction by the Desicion Tree\n",
        "(Q.7., **20 Marks**): Following the completion of decision tree training, the next step is to implement the prediction process through the trained tree structure. To achieve this, we need to create a function named 'tree_prediction.' This function takes two inputs: a test dataframe containing the samples to be predicted and the trained decision tree. It returns the predicted labels generated by the decision tree as a single DataFrame column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yi1bTnb3rEE"
      },
      "outputs": [],
      "source": [
        "def tree_prediction(testing_data, tree_model):\n",
        "\n",
        "  pred_labels = []\n",
        "  # Unpack the tree_model list into three separate variables: tree_connectivity, node_labels, and node_types\n",
        "  [tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "  # Iterate through each sample in the testing_data\n",
        "  for i in range(len(testing_data)):\n",
        "    # Get a sample from the testing dataset\n",
        "    sample = testing_data.loc[i]\n",
        "\n",
        "    # Start at the root node, which is always named \"node_1_1\"\n",
        "    current_node = \"node_1_1\"\n",
        "\n",
        "    #.................................\n",
        "    # write the rest here:\n",
        "    # Begin a loop to traverse the decision tree until a leaf node is reached\n",
        "\n",
        "    while (node_types[current_node] != \"Leaf\"):\n",
        "      #print(f\"i is {i} where sample is {sample} and current node is {current_node}\")\n",
        "\n",
        "      for key, value in tree_connectivity[current_node].items():\n",
        "        #print(f\"key is {key} and value is {value}\")\n",
        "        try:\n",
        "          attr, attr_value = key.split(\"==\")\n",
        "        except:\n",
        "          print(f\"key is {key}\")\n",
        "\n",
        "        #print(f\"{sample[attr]} == {attr_value}\")\n",
        "        if sample[attr] == attr_value:\n",
        "          current_node = value\n",
        "          break\n",
        "\n",
        "      else:\n",
        "        # if no leaf node is reached, select the first child node\n",
        "        for key, value in tree_connectivity[current_node].items():\n",
        "          current_node = value\n",
        "          break\n",
        "    #.................................\n",
        "    # Once a leaf node is reached, append the predicted label to the pred_labels list\n",
        "    pred_labels.append(node_labels[current_node])\n",
        "\n",
        "\n",
        "    # find the node label and put it in the pred_labels Pandas Series\n",
        "\n",
        "    #.................................\n",
        "  # Return the Pandas Series containing the predicted labels\n",
        "  return pd.Series(pred_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtIABZziDgSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4709e2-e3e4-439a-f8bf-dea99e2839e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         acc\n",
            "1       unacc\n",
            "2       unacc\n",
            "3       unacc\n",
            "4       unacc\n",
            "        ...  \n",
            "1395    unacc\n",
            "1396    unacc\n",
            "1397    unacc\n",
            "1398    unacc\n",
            "1399    unacc\n",
            "Length: 1400, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Check your implementation on training dataframe:\n",
        "tree_model = tree_train(train_df, 0.9)\n",
        "pred_labels = tree_prediction(train_df, tree_model)\n",
        "print(pred_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRVHjnEi5lvn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Part 8: Evaluating the Model\n",
        "\n",
        "* (Q.8-a, **5 Marks**)In the final step of this assignment, you'll apply the decision tree learning process. Start by training the decision tree on the training dataset using the 'tree_train' function, setting the terminating threshold to 0.9. Next, employ the 'tree_prediction' function, as previously implemented, to generate predictions for both the training and testing datasets. Following this, your task is to compare these predicted labels with the actual ground-truth labels to compute and report the accuracy rates for both the training and testing datasets.\n",
        "\n",
        "* (Q.8-b, **5 Marks**) Now, repeat the process with a different terminating threshold, specifically 0.7, and once again calculate and report the accuracy rates for the training and testing datasets. Finally, compare and contrast the results obtained with the two different threshold values (0.9 and 0.7). Provide an analysis and discussion of why one threshold might yield higher accuracy compared to the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Jt2EBp6uuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bb05dc-ccf6-41ae-e4c8-517734a30cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " 0.9: train accuracy is 0.9985714285714286\n",
            "\n",
            "\n",
            " 0.9: test accuracy is 0.926605504587156\n",
            "\n",
            "\n",
            " 0.7: train accuracy is 0.7121428571428572\n",
            "\n",
            "\n",
            " 0.7: test accuracy is 0.6513761467889908\n",
            "\n",
            "\n",
            " 0.8: train accuracy is 0.9957142857142857\n",
            "\n",
            "\n",
            " 0.8: test accuracy is 0.9327217125382263\n"
          ]
        }
      ],
      "source": [
        "#.................................\n",
        "# write the rest here:\n",
        "\n",
        "# PART A\n",
        "\n",
        "# use train_df to train a tree model with threshold = 0.9\n",
        "tree_model_A = tree_train(train_df, 0.9)\n",
        "\n",
        "# use the trained tree model to predict the labels of train_df\n",
        "train_pred_labels_A = tree_prediction(train_df, tree_model_A)\n",
        "\n",
        "\n",
        "train_accuracy_A = (train_pred_labels_A == train_df[\"CLASS\"]).mean()\n",
        "\n",
        "print(f\"\\n\\n 0.9: train accuracy is {train_accuracy_A}\")\n",
        "\n",
        "test_pred_labels_A = tree_prediction(test_df, tree_model_A)\n",
        "\n",
        "# compute the accuracy of the predicted labels\n",
        "test_accuracy_A = (test_pred_labels_A == test_df[\"CLASS\"]).mean()\n",
        "\n",
        "print(f\"\\n\\n 0.9: test accuracy is {test_accuracy_A}\")\n",
        "\n",
        "\n",
        "# PART B\n",
        "tree_model_B = tree_train(train_df, 0.7)\n",
        "\n",
        "train_pred_labels_B = tree_prediction(train_df, tree_model_B)\n",
        "\n",
        "train_accuracy_B = (train_pred_labels_B == train_df[\"CLASS\"]).mean()\n",
        "\n",
        "print(f\"\\n\\n 0.7: train accuracy is {train_accuracy_B}\")\n",
        "\n",
        "test_pred_labels_B = tree_prediction(test_df, tree_model_B)\n",
        "\n",
        "# compute the accuracy of the predicted labels\n",
        "test_accuracy_B = (test_pred_labels_B == test_df[\"CLASS\"]).mean()\n",
        "\n",
        "print(f\"\\n\\n 0.7: test accuracy is {test_accuracy_B}\")\n",
        "\n",
        "\n",
        "# PART B Explination: When reducing the threshold from 0.9 to 0.7 our training and testing accuracy take a steep dive; our testing accuracy goes from 92.7% to 65.1%. This is due to the fact that reducing the threshold leads to training an under-fitted model as it isolated the outputs to a lower degree. This happens because when you lower threshold, nodes have a lower threshold they have to meet in order to become a leaf, which makes the model \"simipler\". Meanwhile, a higher threshold makes the model \"detailed\" as it has more splits. When the threshold is too high, it might result in over-fitting where the model is too specific to the training data and does not work well for the testing data. Hence, the ideal threshold will account for both under-fitting and over-fitting. For this specific case, we determined that the ideal threshold that yielded the highest result was ~0.8 which gave us a testing accuracy of about 93.3%.\n",
        "\n",
        "\n",
        "# # Extra Test\n",
        "# tree_model_C = tree_train(train_df, 0.8)\n",
        "\n",
        "# train_pred_labels_C = tree_prediction(train_df, tree_model_C)\n",
        "\n",
        "# train_accuracy_C = (train_pred_labels_C == train_df[\"CLASS\"]).mean()\n",
        "\n",
        "# print(f\"\\n\\n 0.8: train accuracy is {train_accuracy_C}\")\n",
        "\n",
        "# test_pred_labels_C = tree_prediction(test_df, tree_model_C)\n",
        "\n",
        "# # compute the accuracy of the predicted labels\n",
        "\n",
        "# test_accuracy_C = (test_pred_labels_C == test_df[\"CLASS\"]).mean()\n",
        "\n",
        "# print(f\"\\n\\n 0.8: test accuracy is {test_accuracy_C}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8-b Explanation: When reducing the threshold from 0.9 to 0.7 our training and testing accuracy take a steep dive; our testing accuracy goes from 92.7% to 65.1%. This is due to the fact that reducing the threshold leads to training an under-fitted model as it isolated the outputs to a lower degree. This happens because when you lower threshold, nodes have a lower threshold they have to meet in order to become a leaf, which makes the model \"simipler\". Meanwhile, a higher threshold makes the model \"detailed\" as it has more splits. When the threshold is too high, it might result in over-fitting where the model is too specific to the training data and does not work well for the testing data. Hence, the ideal threshold will account for both under-fitting and over-fitting. For this specific case, we determined that the ideal threshold that yielded the highest result was ~0.8 which gave us a testing accuracy of about 93.3%."
      ],
      "metadata": {
        "id": "CLZUr_qCoiVU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}